{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tutorial-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow v2.5.0-dev20210213\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f'tensorflow v{tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arbitrary-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-ground",
   "metadata": {},
   "source": [
    "### Loading Shakespare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-arabic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complete-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spread-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-department",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Since encoding each word will be harder for us, we will encode each unique character as a different integer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "configured-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort each unique character in the text\n",
    "vocab = sorted(set(text)) \n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def encode(text):\n",
    "  return np.array([char2idx[c] for c in text])\n",
    "\n",
    "def decode(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "applied-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "respective-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
      "Decoded: First Citizen\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", encode(text[:13]))\n",
    "print(\"Decoded:\", decode(dataset[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-leather",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sudden-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  \n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verbal-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  \n",
    "    input_text = chunk[:-1]  \n",
    "    target_text = chunk[1:] \n",
    "    return input_text, target_text \n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "waiting-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-syracuse",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specified-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]), # None - dont know how long each one will be\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True, # if False, will only return a single output\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'), # what the values start at\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-deviation",
   "metadata": {},
   "source": [
    "### Loss/Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "talented-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch) \n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupational-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions.shape)\n",
    "pred = example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "advised-suffering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MjmuQS,JSFVcGRp!s?JXEE\\n\\nTKi$UZZqVo3RTID..kGMOftTSnlWoQt'MDPzYeXO3s&;sCQMXRoUzmkQkup:&:L?,3!VwfSmzfEh\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the sample distribution\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "#Reshape the array and convert all the int to number to see the actual character\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "# Decode the array from int to text\n",
    "predicted_chars = decode(sampled_indices)\n",
    "\n",
    "predicted_chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intelligent-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "historic-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "obvious-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-produce",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-bookmark",
   "metadata": {},
   "source": [
    "Train as many epochs as you like, it's not possible to overfit this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pediatric-warehouse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 26s 142ms/step - loss: 2.9674\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 25s 142ms/step - loss: 1.9269\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback, early_stop])\n",
    "# history = model.fit(data, epochs=10, callbacks=[checkpoint_callback, early_stop])\n",
    "# history = model.fit(data, epochs=50, callbacks=[checkpoint_callback, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-pixel",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adopted-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wound-bargain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dominant-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/50_epoch\"))Ëœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "integrated-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a specific checkpoint\n",
    "# checkpoint_num = 10\n",
    "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "# model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-duplicate",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "suffering-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "satisfied-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a starting string: Romeo \n",
      "Romeo pright cail thy tconco's.\n",
      "\n",
      "And,\n",
      "Such sor. Mary!\n",
      "\n",
      "LORCOO:\n",
      "To love\n",
      "Why come, sin a brok lide incuerhrets: bean?\n",
      "\n",
      "LARTIO:\n",
      "In tian? I flat lere o'cl plasing's Clerise: I dederth,\n",
      "And I must I umone with yours in must Bolk,\n",
      "At am growedt, sool more; for the Proforr?\n",
      "'tim, his mist the honstato as diend.\n",
      "\n",
      "SSBANINAN:\n",
      "I to you peepunr:\n",
      "You then os sasensly caling this innels.\n",
      "Where tend thee; My ware, I'll paspears what I dove wonds well,\n",
      "Think wore! Do tod chail dismestimarn,\n",
      "No i atay to be. I mur made me; if Clayem ene\n",
      "Of thould sweateral maringly.\n",
      "\n",
      "Glond I peape up to sue 'dy talk!\n",
      "\n",
      "MARCEASI:\n",
      "The lighes wave I propar Goke, not, fill his langel-was.\n",
      "\n",
      "WAAND CLARUS:\n",
      "To train, myself,\n",
      "And I cone the our brest; hore insiclicial enclais\n",
      "With brighieg, I wous, no the will flanks!\n",
      "I most revell\n",
      "Wo wer\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
