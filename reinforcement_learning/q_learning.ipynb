{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reserved-repository",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q-Learning is a simple yet quite powerful technique in machine learning that involves learning a matrix of action-reward values. This matrix is often reffered to as a Q-Table or Q-Matrix. The matrix is in shape (number of possible states, number of possible actions) where each value at matrix[n, m] represents the agents expected reward given they are in state n and take action m. The Q-learning algorithm defines the way we update the values in the matrix and decide what action to take at each state. The idea is that after a succesful training/learning of this Q-Table/matrix we can determine the action an agent should take in any state by looking at that states row in the matrix and taking the maximium value column as the action.\n",
    "\n",
    "**Consider this example.**\n",
    "\n",
    "Let's say A1-A4 are the possible actions and we have 3 states represented by each row (state 1 - state 3).\n",
    "\n",
    "| A1  | A2  | A3  | A4  |\n",
    "|:--: |:--: |:--: |:--: |\n",
    "|  0  |  0  | 10  |  5  |\n",
    "|  5  | 10  |  0  |  0  |\n",
    "| 10  |  5  |  0  |  0  |\n",
    "\n",
    "If that was our Q-Table/matrix then the following would be the preffered actions in each state.\n",
    "\n",
    "> State 1: A3\n",
    "\n",
    "> State 2: A2\n",
    "\n",
    "> State 3: A1\n",
    "\n",
    "We can see that this is because the values in each of those columns are the highest for those states!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-jersey",
   "metadata": {},
   "source": [
    "### Learning the Q-Table\n",
    "\n",
    "I'll start by noting that our Q-Table starts of with all 0 values. This is because the agent has yet to learn anything about the enviornment. \n",
    "\n",
    "Our agent learns by exploring the enviornment and observing the outcome/reward from each action it takes in each state. But how does it know what action to take in each state? There are two ways that our agent can decide on which action to take.\n",
    "1. Randomly picking a valid action\n",
    "2. Using the current Q-Table to find the best action.\n",
    "\n",
    "Near the beginning of our agents learning it will mostly take random actions in order to explore the enviornment and enter many different states. As it starts to explore more of the enviornment it will start to gradually rely more on it's learned values (Q-Table) to take actions. This means that as our agent explores more of the enviornment it will develop a better understanding and start to take \"correct\" or better actions more often. It's important that the agent has a good balance of taking random actions and using learned values to ensure it does get trapped in a local maximum. \n",
    "\n",
    "After each new action our agent wil record the new state (if any) that it has entered and the reward that it recieved from taking that action. These values will be used to update the Q-Table. The agent will stop taking new actions only once a certain time limit is reached or it has acheived the goal or reached the end of the enviornment. \n",
    "\n",
    "#### Updating Q-Values\n",
    "The formula for updating the Q-Table after each action is as follows:\n",
    "> $ Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action]) $\n",
    "\n",
    "- $\\alpha$ stands for the **Learning Rate**\n",
    "\n",
    "- $\\gamma$ stands for the **Discount Factor**\n",
    "\n",
    "#### Learning Rate $\\alpha$\n",
    "The learning rate $\\alpha$ is a numeric constant that defines how much change is permitted on each QTable update. A high learning rate means that each update will introduce a large change to the current state-action value. A small learning rate means that each update has a more subtle change. Modifying the learning rate will change how the agent explores the enviornment and how quickly it determines the final values in the QTable.\n",
    "\n",
    "#### Discount Factor $\\gamma$\n",
    "Discount factor also know as gamma ($\\gamma$) is used to balance how much focus is put on the current and future reward. A high discount factor means that future rewards will be considered more heavily.\n",
    "\n",
    "<br/>\n",
    "<p>To perform updates on this table we will let the agent explpore the enviornment for a certain period of time and use each of its actions to make an update. Slowly we should start to notice the agent learning and choosing better actions. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/chung/.local/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /home/chung/.local/lib/python3.8/site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/chung/.local/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/chung/.local/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/chung/.local/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/chung/.local/lib/python3.8/site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: future in /home/chung/.local/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abstract-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # Open AI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "average-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n)   # Get number of states\n",
    "print(env.action_space.n)   # Get number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-stage",
   "metadata": {},
   "source": [
    "Reset enviornment to default state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excited-addition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-driving",
   "metadata": {},
   "source": [
    "Get a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pressed-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-berkeley",
   "metadata": {},
   "source": [
    "Take action, notice it returns information about the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handed-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-buyer",
   "metadata": {},
   "source": [
    "Render the GUI for the enviornment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coordinated-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-chambers",
   "metadata": {},
   "source": [
    "### Frozen Lake Enviornment\n",
    "Now that we have a basic understanding of how the gym enviornment works it's time to discuss the specific problem we will be solving.\n",
    "\n",
    "The enviornment we loaded above ```FrozenLake-v0``` is one of the simplest enviornments in Open AI Gym. The goal of the agent is to navigate a frozen lake and find the Goal without falling through the ice (render the enviornment above to see an example).\n",
    "\n",
    "There are:\n",
    "- 16 states (one for each square) \n",
    "- 4 possible actions (LEFT, RIGHT, DOWN, UP)\n",
    "- 4 different types of blocks (F: frozen, H: hole, S: start, G: goal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-appearance",
   "metadata": {},
   "source": [
    "### Building the Q-Table\n",
    "\n",
    "```\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "STATES = env.observation_space.n\n",
    "ACTIONS = env.action_space.n\n",
    "\n",
    "EPISODES = 2000 # How many times to run the enviornment from the beginning\n",
    "MAX_STEPS = 100  # Max number of steps allowed for each run of enviornment\n",
    "\n",
    "LEARNING_RATE = 0.81  # Learning rate\n",
    "GAMMA = 0.96\n",
    "\n",
    "Q = np.zeros((STATES, ACTIONS))  # Create a matrix with all 0 values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-daily",
   "metadata": {},
   "source": [
    "### Picking an Action\n",
    "Remember that we can pick an action using one of two methods:\n",
    "1. Randomly picking a valid action\n",
    "2. Using the current Q-Table to find the best action.\n",
    "\n",
    "Here we will define a new value $\\epsilon$ that will tell us the probabillity of selecting a random action. This value will start off very high and slowly decrease as the agent learns more about the enviornment.\n",
    "\n",
    "```\n",
    "epsilon = 0.9  # Start with a 90% chance of picking a random action\n",
    "\n",
    "# Code to pick action\n",
    "if np.random.uniform(0, 1) < epsilon:  # We will check if a randomly selected value is less than epsilon.\n",
    "    action = env.action_space.sample()  # Take random action\n",
    "else:\n",
    "    action = np.argmax(Q[STATES, :])  # Use Q table to pick best action based on current values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-runner",
   "metadata": {},
   "source": [
    "### Updating Q Values\n",
    "\n",
    "`Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-cigarette",
   "metadata": {},
   "source": [
    "### Combine these togther to create our Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "average-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "STATES = env.observation_space.n\n",
    "ACTIONS = env.action_space.n\n",
    "\n",
    "Q = np.zeros((STATES, ACTIONS))\n",
    "\n",
    "EPISODES = 10000 # How many times to run the enviornment from the beginning\n",
    "MAX_STEPS = 200  # Max number of steps allowed for each run of enviornment\n",
    "LEARNING_RATE = 0.80  # Learning rate\n",
    "GAMMA = 0.96\n",
    "EPSILON = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constitutional-hospital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46067631e-01 1.64388185e-02 1.65532527e-02 1.61095868e-02]\n",
      " [4.11611607e-03 4.90197065e-03 6.51381115e-03 1.52122520e-01]\n",
      " [3.85464587e-03 3.07265883e-03 4.46418417e-03 1.88921470e-01]\n",
      " [4.25644332e-03 5.33202218e-03 3.50520269e-03 1.22954551e-01]\n",
      " [1.59770068e-01 1.54450344e-02 1.41698856e-02 9.60969234e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.04763038e-01 9.14138395e-06 5.26206030e-06 5.19298725e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.99727582e-03 3.72307148e-03 1.05998704e-02 2.40756938e-01]\n",
      " [3.67472390e-03 4.00072600e-01 5.39536251e-03 3.98295876e-03]\n",
      " [4.96272681e-01 1.82027331e-03 1.28104085e-03 7.65834158e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.27267566e-02 5.47873363e-02 6.24692649e-01 7.27480839e-02]\n",
      " [1.39180072e-01 9.41964825e-01 1.76808472e-01 1.63851840e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "Average reward: 0.679:\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(EPISODES):\n",
    "  state = env.reset()\n",
    "  for _ in range(MAX_STEPS):\n",
    "    if np.random.uniform(0, 1) < EPSILON:\n",
    "      action = env.action_space.sample()  \n",
    "    else:\n",
    "      action = np.argmax(Q[state, :])\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action) # next_state, reward, done, info\n",
    "    Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
    "    state = next_state\n",
    "\n",
    "    if done: \n",
    "      rewards.append(reward)\n",
    "      EPSILON -= 0.001\n",
    "      break  # Reached goal\n",
    "\n",
    "print(Q)\n",
    "print(f\"Average reward: {sum(rewards)/len(rewards)}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upset-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_average(values):\n",
    "  return sum(values)/len(values)\n",
    "\n",
    "avg_rewards = []\n",
    "for i in range(0, len(rewards), 1000):\n",
    "  avg_rewards.append(get_average(rewards[i:i+1000])) \n",
    "\n",
    "plt.plot(avg_rewards)\n",
    "plt.ylabel('average reward')\n",
    "plt.xlabel('episodes (1000\\'s)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
